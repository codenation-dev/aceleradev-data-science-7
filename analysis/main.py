# -*- coding: utf-8 -*-
"""main (copy).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ad5L5rr9NvXayOHoDHErDkbQud4TARh8

# ***Video da apresentação:***

---


# https://youtu.be/-5xjHpiqnL0 **bold text**
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/gdrive')
# %cd /gdrive

'''!pip install icc_rt
#!pip uninstall icc_rt
'''

import pandas as pd
import numpy as np
import gensim 
import multiprocessing
import sklearn.preprocessing as pp
import warnings
import logging  # Setting up the loggings to monitor gensim
import numba
from IPython.display import display
from IPython.core.display import HTML
from gensim.models import Word2Vec,KeyedVectors 
from time import time
from gensim.models.phrases import Phrases, Phraser
from sklearn.feature_extraction.text import TfidfVectorizer
warnings.filterwarnings('ignore')

#logging.basicConfig(format="%(levelname)s - %(asctime)s: %(message)s", datefmt= '%H:%M:%S', level=logging.INFO)

t = time()
estaticos_market = pd.read_csv('/gdrive/My Drive/estaticos_market.csv')
#estaticos_market = pd.read_csv('/gdrive/My Drive/estaticos_portfolio1.csv')
print(estaticos_market.shape)
print('Time to Read Csv: {} mins'.format(round((time() - t) / 60, 2)))

percent_missing = estaticos_market.isnull().sum() * 100 / len(estaticos_market)
missing_value_df = pd.DataFrame({'percent_missing': percent_missing,'dtypes': estaticos_market.dtypes,})
missing_value_df.sort_values('percent_missing', inplace=True)
missing_value_df.head(181)

@numba.jit()
def fast_clean(df):
  t = time()
  col_exclude = ["fl_email", "fl_telefone", "qt_filiais", "tx_rotatividade", "tx_crescimento_24meses", "tx_crescimento_12meses", "qt_funcionarios_12meses", 
              "qt_funcionarios_24meses", "min_meses_servicos",'Unnamed: 0','fl_matriz','qt_alteracao_socio_total','qt_alteracao_socio_365d','qt_alteracao_socio_90d','grau_instrucao_macro_desconhecido']
  df = df.drop(col_exclude, axis=1)
  booleandf = df.select_dtypes(include=[bool]).columns
  booleanDictionary = {True: 'T', False: 'F'}
  for column in booleandf:
    df[column] = df[column].map(booleanDictionary)
  df= df.astype(str)
  objectdf  = df.select_dtypes(include=[object]).columns
  for column in objectdf:
    df[column] = df[column].str.replace(' ', '')
    df[column] = np.where(df[column] == 'nan', str(df.index), df[column])
    df[column] = column.replace('_', '')+"_"+df[column]
    df[column] = df[column].str.replace(',', '')
  Maker_Model = list(df.id.unique()) 
  indice = pd.Series(df.index, index=df['id']).drop_duplicates()
  df_id = pd.DataFrame({'id': df['id']})
  df = df.drop(['id'],axis=1)
  df['id'] = df_id['id']
  df2 = df_join(df) 
  df_clean = pd.DataFrame({'clean': df2})
  sent = [row.split(',') for row in  df_clean['clean']]
  print('Time to clean: {} mins'.format(round((time() - t) / 60, 2)))
  return(sent,indice,Maker_Model,df_clean)
def df_join(df):
  df2 = df.apply(lambda x: ','.join(x.astype(str)), axis=1)
  return(df2)

sent,indice, Maker_Model,df_clean = fast_clean(estaticos_market)

""" - UM DOS TESTE REALIZADOS - NÃO TEVE UM BOM DESEMPENHO
t = time()
tfidf = TfidfVectorizer(sublinear_tf=False,stop_words=None)
tfidf_matrix = tfidf.fit_transform(df_clean['clean'])
print('Time to tfidf: {} mins'.format(round((time() - t) / 60, 2)))
x = (tfidf_matrix.getrow(554).toarray().flatten())
y = (tfidf_matrix.getrow(9).toarray().flatten())
#x = np.squeeze(np.asarray(x))
#y = np.squeeze(np.asarray(y))
result = fast_cosine(x, y)
print(result)
tfidf_matrix = pp.normalize(tfidf_matrix.tocsc(), axis=0)
tfidf_matrix.shape
from scipy import sparse
b = (tfidf_matrix[9].transpose().todense())
b = np.squeeze(np.array(b), axis=1) 
len(b)
"""

t = time()
cores=5
model = Word2Vec(sent,min_count=1,size=300,workers=cores, window=3, sg = 0)
print('Time to build model: {} mins'.format(round((time() - t) / 60, 2)))

t = time()
model.wv.save_word2vec_format('/gdrive/My Drive/model.bin', binary=True)
print('Time to save: {} mins'.format(round((time() - t) / 60, 2)))

'''
t = time()
model2 = KeyedVectors.load_word2vec_format('/gdrive/My Drive/model.bin', binary=True)
#print('Time to load: {} mins'.format(round((time() - t) / 60, 2)))'''

def cosine_distance (model, word,target_list,num) :
    cosine_dict ={}
    try:
      a = model[word]
      for item in target_list :
          b = model[item]
          type(b)
          cos_sim = fast_cosine(a, b)
          cosine_dict[item] = cos_sim
      dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order 
      data_day_list = []
      count = 1
      for item in dist_sort:
          data_day =(estaticos_market[estaticos_market.id==item[0][3:]])
          data_day_list.append(data_day)
          if count==num:
              break
          count=count+1
      final_data_day = pd.concat(data_day_list)
    except KeyError:
      print("")
    return final_data_day
@numba.jit()
def cosine_distance2 (tfidf_matrix,word,indice,target_list,num) :
    cosine_dict ={}
    word_list = []
    idx = indice[word]
    a = (tfidf_matrix.getrow(554).toarray().flatten())
    for item in target_list :
        if item != word :
            idx = indice[item]
            b = (tfidf_matrix.getrow(idx).toarray().flatten())
            b = np.squeeze(np.array(b))  
            cos_sim = fast_cosine(a, b)
            cosine_dict[item] = cos_sim
    dict_sorted(cosine_dict)
    data_day_list = []
    count = 1
    for item in dist_sort:
        print(item)
        data_day =(estaticos_market[estaticos_market.id==item[0][3:]])
        data_day_list.append(data_day)
        if count==num:
            break
        count=count+1
    final_data_day = pd.concat(data_day_list)
    return final_data_day
def dict_sorted(cosine_dict):
     dist_sort=sorted(cosine_dict.items(), key=lambda dist: dist[1],reverse = True) ## in Descedning order 
     return(dist_sort)
@numba.jit(('f8,f8'),parallel=True,fastmath=True)
def fast_cosine(a, b):
    result  = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))
    return(result)
@numba.jit(parallel=True,fastmath=False)
def requerimento(element,quant):
  t = time()
  result_list=[]
  element
  result = cosine_distance (model,element,Maker_Model,quant);
  print('Tempo de cálculo {} mins'.format(round((time() - t) / 60, 2)))
  return(result)

'''BLOCO DE TESTE
#id='id_dabe79bec87c88ae04e869bf6bd321ee5e1893cecf66255be9ea861a9e5841a9'
id='id_7d58990ba786a3a26617a8afbf814a9919ab34aa09013a559b2c9d45eb439720'
#id='id_fc6969bffd9f104e3a148ad7df64b338ca885dd6a5aa5153b4754bd55746d638'
#a = model[id]
#model2.most_similar([id])
#cosine_distance (model,id,Maker_Model,10)
cosine_distance2 (tfidf_matrix,id,indice,Maker_Model,10)
#result = model2.similar_by_vector('id_'+x[0], topn= 50)
#data_day_list=[]
'''

estaticos_portfolio1 = pd.read_csv('/gdrive/My Drive/estaticos_portfolio1.csv')
estaticos_portfolio2 = pd.read_csv('/gdrive/My Drive/estaticos_portfolio2.csv')
estaticos_portfolio3 = pd.read_csv('/gdrive/My Drive/estaticos_portfolio3.csv')
print(estaticos_portfolio1.shape)
print(estaticos_portfolio2.shape)
print(estaticos_portfolio3.shape)
requis=(['7d58990ba786a3a26617a8afbf814a9919ab34aa09013a559b2c9d45eb439720','70485e6b8abe52d8fb4e0bf060fb6f0f4f8576cb7583d885a8dcac38f506389','6fecd2c150c5fe474b31b2b4b68e5d7dc9d22f51dd5c781d801b58102a71a570'])
#requis=estaticos_portfolio3['id']
result_list=[]
result_list2=[]
for element in requis:
  try:
    id = 'id_'+element
    print(id)
    model[id]
    result=(requerimento(id,50))
    result_list.append(result)
  except Exception as e:
     print('chave não encontrada')
result_final = pd.concat(result_list)
result_final.drop_duplicates(keep = 'first', inplace = True)
result_list2 = []
for element in requis:
    result = (result_final[result_final.id==element]) 
    result_list2.append(result)
result_final2 = pd.concat(result_list2)
i1 = len(requis)
f1,f2 = result_final2.shape
print("Percentual de item solicitado no resultado: "+str(round(f1/i1*100,2))+'%')
pd.options.display.max_columns = None
display(result_final)